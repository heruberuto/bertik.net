---
title: "Assessing the quality of information extraction"
pubDatetime: 2024-04-08T00:00:00Z
description: "arXiv preprint arXiv:2404.04068"
tags: ["publications", "information-extraction"]
featured: false
kind: "publication"
authors:
  - "F Seitl"
  - "T Kovářík"
  - "S Mirshahi"
  - "J Kryštůfek"
  - "R Dujava"
  - "M Ondreička"
  - "et al."
venue: "arXiv preprint arXiv:2404.04068"
citations: 11
---

**Authors:** F Seitl, T Kovářík, S Mirshahi, J Kryštůfek, R Dujava, M Ondreička, et al.  
**Venue:** arXiv preprint arXiv:2404.04068  
**Year:** 2024  
**Citations:** 11

## Links

- arXiv: [2404.04068](https://arxiv.org/abs/2404.04068)
- Google Scholar: [View profile](https://scholar.google.com/citations?user=1AZsqXEAAAAJ)

## Abstract

This work studies how to evaluate information extraction outputs beyond simple exact matching. We discuss practical quality dimensions (correctness, completeness, consistency, and usefulness), compare metric choices, and show where automatic scores diverge from human judgment for real extraction tasks.

## Resources

- Video: _TODO_
- Slides: _TODO_
- Code: _TODO_
- Dataset: _TODO_

## Notes

I like this paper as a "methodology anchor" for later projects: before optimizing extraction models, we first make evaluation explicit and reliable. That framing helped us design cleaner experiments in downstream fact-checking and claim verification pipelines.
